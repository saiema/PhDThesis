%!TEX root = main.tex
\chapter{Introducci\'on}
\label{cap:introduccion}
En la actualidad, el software se encuentra en todos los aspectos de la vida diaria, y en general la expectativa del usuario com\'un es confiar en que el mismo simplemente funciona. Nadie espera que el auto no arranque por un error en el software embebido que utiliza, el cajero de un banco no se espera que de pronto no retorne billetes al realizar una extracci\'on, que el reloj del celular marque un horario incorrecto, etc. Sin embargo, en 2015, se descubri\'o un bug en el modelo Boing 787 Dreamliner que pod\'ia causar el apagado de todos los generadores el\'ectricos del avi\'on si \'este permanec\'ia encendido por m\'as de 248 d\'ias; En los 80, un error en el c\'odigo del controlador para la m\'aquina de terapia de radiaci\'on, \emph{Therac-25} caus\'o la muerte de varios pacientes al administrar cantidades excesivas de radiaci\'on beta; En 2018 un bug en WhatsApp causaba que si se recib\'ia un mensaje en Unicode conteniendo una secuencia repitiendo el caract\'er especial que sobreescribe la direcci\'on del texto, la aplicaci\'on y en muchos casos el dispositivo completo dejara de responder.
%REF: BUG AVION: https://s3.amazonaws.com/public-inspection.federalregister.gov/2015-10066.pdf
%REF: THREAC-25, CITATION NEEDED?
%REF: BLACK DOT OF DEATH, CITATION NEEDED?
Claramente, el software no simplemente funciona siempre y esto demuestra la necesidad de evaluar el mismo para lograr una confianza razonable de que el mismo funciona correctamente. El problema, es poder evaluar, dado un software, que el mismo realiza de manera correcta las tareas para las cuales fue desarrollado. Intuitivamente y algo que se realiza en la actualidad, es ejecutar el software bajo ciertos escenarios, los cuales consisten de entradas directas e indirectas, y comprobar que el resultado es el esperado, para evaluar que el software es correcto. Un problema con este enfoque es que salvo que se prueben todos los escenarios posibles, la evaluaci\'on no es completa. Evidentemente, salvo para programas triviales, evaluar bajo todos los escenarios posibles es inviable por lo cual un subconjunto de \'estos es necesario. Cuantos y cuales de todos los escenarios se seleccionan est\'a directamente relacionado con la confianza de que, en caso de no encontrar fallas, el software sea correcto.

No solamente es necesario un conjunto de escenarios, el comportamiento esperado requiere ser definido, y la forma m\'as simple es usando un valor esperado, por ejemplo, esperar que llamar a un m\'etodo \texttt{helloWorld()} me retorne la cadena \emph{"Hello World"}; un ejemplo m\'as complejo es que luego de ejecutar el m\'etodo \texttt{insert(3)} sobre una lista vac\'ia, llamar al m\'etodo \texttt{contains(3)} retorne verdadero. Otra forma de evaluar el comportamiento esperado es evaluando propiedades como que para una lista simplemente encadenada, luego de ejecutar un m\'etodo particular, la misma est\'a ordenada. Lo que define el comportamiento esperado es lo que se conoce como \emph{Or\'aculo}.

Podemos entonces definir a un test como una serie de pasos conteniendo tres partes principales: \emph{Preparaci\'on}(Arrange), consiste en definir o construir el escenario sobre el que se va a evaluar un programa; \emph{Ejecuci\'on}(Act), donde se va a ejecutar el programa a evaluar; \emph{Evaluaci\'on}(Assert), donde se va a evaluar el resultado obtenido contra el esperado.

Un conjunto de tests debe entonces, ser finito, por que no es posible utilizar todos los escenarios posibles; su ejecuci\'on debe requerir recursos razonables, donde estos van a depender del contexto y los recursos disponibles, aunque en general es esperable que al menos haya un conjunto de tests que puedan ser ejecutados de manera r\'apida como apoyo al desarrollador; finalmente el \'exito en la execuci\'on de estos tests, es decir que ninguno falle, deber\'ia servir como control de calidad del software.

Es necesario entonces un criterio para evaluar la calidad de un conjunto de tests, si bien, intuitivamente, evaluar que tan bueno es un test suite detectando fallas parece ser un buen criterio, salvo para fallas conocidas, no es posible realizar esta evaluaci\'on. La raz\'on de esto es que las fallas se definen en t\'erminos de su reparaci\'on, por ejemplo, \emph{Falta incrementar la variable \texttt{i} al recorrer el arreglo}, define la falla en t\'erminos de que es necesario para repararla, no solo eso, sin\'o que se define en base a una posible reparaci\'on. Criterios indirectos son entonces necesarios, \'estos definen metas a cubrir por los tests y eval\'uan cuantas son finalmente cubiertas. Estos criterios de cobertura se suelen dividir en dos categor\'ias principales: \emph{Caja blanca}(White box), cuando las metas a cubrir se basan en la estructura del programa; \emph{Caja negra}(Black box), cuando las metas se basan en las especificaciones. Ejecutar todas las sentencias en un programa, \emph{Cobertura de sentencias}, ejecutar todas las alternativas para las sentencias de control de flujo, \emph{Cobertura de ramas}, son ejemplos de caja blanca; determinar clases de equivalencia para las entradas del programa basado en su especificaci\'on y tener al menos un escenario por cada una de estas clases es un ejemplo de caja negra.

Un ejemplo de como los criterios de evaluaci\'on de test suite intentan dar una medida indirecta de la capacidad del mismo en detectar fallas potenciales, se puede apreciar en la cobertura de sentencias, este criterio impone como metas a cubrir, la ejecuci\'on de cada una de las sentencias del programa bajo evaluaci\'on. La intuici\'on del este criterio es bastante simple, lo m\'inimo y necesario para descubrir una falla, es ejecutar la sentencia o sentencias en donde se encuentra. Es sin embargo f\'acil de encontrar ejemplos de fallas simples en donde este criterio da una evaluaci\'on positiva a un conjunto de tests que es incapaz de detectar estos casos. Varios de estos ejemplos pueden ser detectados por un test suite que tenga una buena evaluaci\'on de parte del criterio de cobertura de ramas. Esto lleva a que evidentemente hay criterios que generan m\'as confianza que otros. Lo que lleva a la idea de evaluar un test suite bajo su habilidad de detectar fallas artificiales bajo el razonamiento de que los programadores suelen crear programas que cuando tienen fallas, el programa no est\'a lejos de la soluci\'on correcta \cite{bibliography.mutation.DeMillo}, y peque\~nos cambios sint\'acticos en un programa deber\'ian emular los errores que se suelen cometer en su desarrollo.

\emph{Mutation testing}, es un criterio que genera copias del programa original en donde cada uno tiene inyectada una falla artificial, \emph{mutante}, en forma de un cambio sint\'actico simple, \emph{mutaci\'on}. Por cada mutante se ejecutan los tests, si al menos uno de estos falla, entonces se marca al mutante como detectado. El valor asociado a este criterio es la relaci\'on entre mutantes detectados y todos los mutantes, \emph{mutation score}. Las fallas artificiales generadas por mutation testing est\'an basadas en distintos operadores de mutaci\'on que definen familias de cambios sint\'acticos similares. Por ejemplo, dada una expresi\'on relacional binaria, cambiar el operador de relaci\'on por cada uno de los existentes en el lenguaje en el cual el programa est\'a desarrollado, es un operador de mutaci\'on. Numerosos estudios han intentado responder si existe una correlaci\'on (acoplamiento) entre la capacidad de un test suite en detectar fallas artificiales utilizadas en mutation testing y la capacidad de hacerlo para fallas reales, entre ellos, \cite{bibliography.mutation.evaluation.coupling.Offutt89, bibliography.mutation.evaluation.coupling.Offutt92, bibliography.mutation.evaluation.HAndrews05, bibliography.mutation.evaluation.valid-substitute}, estos estudios han encontrado que de hecho existe una correlaci\'on, aunque siempre acotado a casos de estudio particulares. A\'un teniendo en cuenta estos resultados, el rendimiento del criterio est\'a directamente relacionado a los operadores de mutaci\'on utilizados y las fallas artificiales que estos generan. Por un lado la cantidad de mutantes impacta en los recursos necesarios para ejecutar el an\'alisis, ya que en el peor caso es necesario ejecutar todos los tests para cada uno de los mutantes. Por el otro lado, existen fallas artificiales que son trivialmente detectables, por ejemplo, modificar el \'indice en el acceso a un arreglo a un valor negativo, y aquellas que son equivalente al programa original. En el primer caso, ejecutar la sentencia va a causar un error, y cualquier test que lo haga va a detectar dicho mutante, este tipo de fallas triviales van a aumentar el mutation score sin realmente significar un aumento en la calidad del test suite. En el segundo caso, no existe ning\'un escenario para el cual el comportamiento del mutante difiera del comportamiento observable del programa original, en este caso el valor del mutation score va a decrementar sin significar un empeoramiento en el test suite. Finalmente as\'i como las fallas artificiales pueden estar acopladas a fallas reales, el mismo fen\'omeno puede ocurrir entre los mutantes, es decir, detectar ciertos mutantes va a implicar que otros sean tambi\'en detectados. Este caso de acoplamiento entre mutantes lleva a un aumento del mutation score que si bien puede estar asociado a fallas artificiales no triviales, son fallas similares que no implican que el test suite sea capaz de detectar mayor variedad de fallas reales.

Existen varios estudios que intentan atacar los problemas mencionados anteriormente, desde aquellos que lo hacen seleccionando un conjunto \emph{suficiente} de operadores de mutaci\'on, entre otros \cite{bibliography.mutation.selection.Offutt96, bibliography.mutation.selection.ASN2008}; buscando m\'etodos de detectar mutantes equivalentes, \cite{biblography.mutation.evaluation.equivalent.Grun+09, biblography.mutation.evaluation.equivalent.Schuler+10, biblography.mutation.evaluation.equivalent.Schuler+13, biblography.mutation.evaluation.equivalent.Just+13}; y utilizando combinaci\'on de mutaciones para generar mutantes sut\'iles bajo el razonamiento de que \'estos deber\'ian representar fallas m\'as dif\'iciles de detectar, \cite{bibliography.mutation.highorder.Jia+08, bibliography.mutation.highorder.Jia+09, bibliography.mutation.highorder.Harman+11}.

%Metodológicamente, explicar en qué consiste tener una etapa de captura y especificación de requisitos concreta en el proceso de desarrollo.

%Etapa de análisis = captura y especificación de requisitos. Con qué contamos para hacerlo.Y captura? Varias herramientas vinculadas al “proceso”. Ej.: checklists, listas de preguntas frecuentes sobre el software a construir, etc. 

%Con qué contamos para hacerlo. Lenguaje natural, lenguajes de propósito específico (todo esto es especificación).
%Algunas herramientas metodológicas: Diagramas de estímulo/respuesta, DFDs y descomposición de funcionalidades, 4-variable model, KAOS. 

%natural 

%informales 

%Lenguaje natural vs. lenguajes formales para la especificación de requisitos. Sintaxis formal vs sintaxis+semántica. Posibilidades de análisis “objetivo” de specs, libre de interpretaciones subjetivas de las specs por parte de los ingenieros . Algunos antecedentes (mencionados superficialmente).

%Sintaxis + semántica formales facilitan análisis (semi-)automático vinculado a specs de requisitos. Qué se puede sistematizar? (algunos ejemplos de tareas).  Automático vs. asistido. Ejemplos de alto nivel, y mencionando sólo referencias, no trabajos concretos.
%Algunos ejemplos de trabajos explotando estas observaciones: referencias, descripciones superficiales.

%semi-automaticas 

%automaticas

% % % %


\section{Motivaci\'on y Objetivos}
\label{sec:intro.objetivos}
% RE etapa mas informal del proceso de desarrollo
% incertidumbre, desconocimiento, parcialidad.

%[operacional vs declarativo]

%Objetivos

\section{Estado del Arte}
\label{sec:intro.estado-del-arte}
%TODO: Describir brevemente diferentes tipos an\'alisis autom\'aticos que las herramientas y/o metodolog\'ias del estado-del-arte proveen. Mencionar fortalezas y debilidades.

\section{Contribuciones}
\label{sec:intro.contribuciones}
%TODO: Describir porque nuestras t\'ecnicas mejoran o complementan las t\'ecnicas de an\'alisis autom\'aticos existentes. Cuales y porque son los casos de estudios utilizados para validar cada una de las t\'ecnicas.
%\begin{itemize}
%\item operacionalizacion
%\item amplitud de objetivos (liveness)
%\item abstraccion
%\item escalabilidad
%\item herramientas
%\end{itemize}

%Se pueden mencionar papers publicados


\section{Organizaci\'on}
\label{sec:intro.organizacion}
%TODO: Cuales van a ser los temas de cada capitulo desarrollado en este trabajo.


